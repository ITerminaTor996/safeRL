# Safe RL Drone 配置文件 v3.0
# ============================================================
# 核心理念：
# - Safety Specification: 形式化保证（硬约束，零违规）
# - Task Specification: RL 学习（软目标，Robustness 引导）
# - Goal-Conditioned Policy: 泛化到任意目标
# ============================================================

# 环境设置
environment:
  map_path: "maps/map4_large.txt"  # 地图文件（离散环境）
  render: false         # 是否渲染画面（WSL 建议 false）
  view_size: 5          # 局部视野大小（奇数）
  random_goal: true     # 训练时随机采样目标
  random_start: true    # 训练时随机采样起点
  max_episode_steps: 1000  # 每回合最大步数
  dynamic_obstacles: true  # 是否启用动态障碍物
  obstacle_change_prob: 0.01  # 每步障碍物变化概率

# 原子命题定义
propositions:
  # 环境自动提供的命题
  wall: "auto"
  goal: "auto"
  boundary: "auto"
  
  # 用户自定义命题（可选）
  # wp1:
  #   type: "position"
  #   pos: [5, 5]
  # wp2:
  #   type: "position"
  #   pos: [10, 10]

# 规范定义（核心）
specifications:
  # 安全规范（硬约束，通过 SafetyFilter 强制保证）
  safety:
    enabled: true
    formula: "G(!wall) & G(!boundary)"
    filter_method: "enumeration"  # 当前：enumeration（枚举），未来：geometric, cbf
    # 支持的算子: G, F, X, U, &, |, !, ->
    # 示例:
    #   "G(!wall)": 永远不撞墙
    #   "G(!wall) & G(!boundary)": 不撞墙且不越界
    
  # 任务规范（软引导，通过 TaskRewardShaper 引导 RL）
  task:
    enabled: true
    formula: "F(goal)"  # STL 公式
    # 复杂任务示例:
    #   "F(wp1) & F(wp2) & F(goal)": 顺序任务
    #   "F(goal1) | F(goal2)": 多目标选择

# 奖励设计
reward:
  # 环境基础奖励
  goal_reached: 10.0    # 到达目标
  step_penalty: -0.05   # 每步惩罚
  safety_violation: -1.0  # 安全违规（理论上不会触发）
  
  # 任务奖励塑形权重（来自 TaskRewardShaper）
  task_shaping_weights:
    robustness: 0.1     # Robustness 增量奖励（稠密引导）
    progress: 1.0       # FSA 进度奖励（稀疏里程碑）
    acceptance: 10.0    # 任务完成奖励（最终目标）
    trap: 1.0           # 陷阱状态惩罚（任务失败，-10.0）
    filter: 1.0         # 过滤器惩罚（学习安全，-0.5）
    time: 1.0           # 时间惩罚（鼓励效率，-0.01）

# 训练设置
training:
  enabled: true              # 是否进行训练
  algorithm: "PPO"           # 强化学习算法
  total_timesteps: 500000    # 训练总步数（20×20地图需要更多）
  device: "cpu"              # 训练设备: cpu 或 cuda
  verbose: 1                 # 日志详细程度

# 测试设置
testing:
  episodes: 5                # 测试回合数
  deterministic: true        # 是否使用确定性策略
  # 测试目标列表（验证泛化性）
  # 留空则使用随机目标
  test_goals: []             # 例如: [[2,3], [4,1], [3,5]]

# 模型设置
model:
  save_path: "models/"       # 模型保存路径
  auto_save: true            # 训练后自动保存
  load_path: "/mnt/e/RL+形式化方法/models/ppo_safe"               # 加载已有模型路径（留空则创建新模型）/mnt/e/RL+形式化方法/models/ppo_safe
