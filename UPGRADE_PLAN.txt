================================================================================
Safe RL Drone 项目升级方案
基于论文: A Formal Methods Approach to Interpretable Reinforcement Learning
================================================================================

一、项目定位与核心理念
================================================================================

1.1 核心思想
------------
将形式化方法与强化学习结合，实现：
- 形式化方法：保证安全（Safety）—— 硬约束，绝对不可违反
- 强化学习：完成任务（Task）—— 软目标，策略可灵活学习

1.2 与论文的区别
----------------
论文做法：Safety 和 Task 都用形式化方法（CBF/CLF）约束
我们的做法：只用形式化方法约束 Safety，Task 交给 RL 自由学习

理由：
- 提高泛化性能：网络学习"如何在安全约束下完成任务"，而非"按固定路线走"
- 职责分离：安全是绝对的，任务策略是灵活的
- 更符合实际：无人机绝对不能撞墙，但到达目标的路径可以有多种

1.3 环境特点
------------
论文：连续状态空间，使用 TLTL（Truncated Linear Temporal Logic）
我们：离散 GridWorld，使用 LTL，但借鉴 TLTL 的 Robustness 思想

解决方案：在离散环境中定义"伪连续"Robustness（基于距离）


二、系统架构设计
================================================================================

2.1 整体架构
------------

┌─────────────────────────────────────────────────────────────────┐
│                        用户配置层                                │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │  Safety 公式     │  │  原子命题定义    │  │   地图文件       │  │
│  │  G(!wall)       │  │  wall, goal...  │  │   map.txt       │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                        LTL 核心模块                              │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │  公式解析器      │  │  FSA 生成器     │  │  Robustness     │  │
│  │  (Spot 库)      │  │  (自动机)       │  │  计算器          │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                        安全层 (Safety Layer)                     │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │  Safety Monitor：监控 LTL Safety 公式                    │    │
│  │  Action Filter：过滤/替换不安全动作                      │    │
│  │  Robustness Reward：计算安全相关奖励                     │    │
│  └─────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                        强化学习层                                │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │  GridWorld Env  │  │  PPO Agent      │  │  Task Reward    │  │
│  │  (环境)         │  │  (策略网络)      │  │  (任务奖励)      │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────┘


2.2 数据流
----------

1. Agent 选择动作 a
2. Safety Layer 检查动作是否安全
   - 安全 → 执行 a
   - 不安全 → 替换为安全动作 a'，记录干预
3. 环境执行动作，返回 (s', r_env, done, info)
4. 计算总奖励：r = r_task + r_safety + r_robustness
5. RL Agent 学习


三、LTL 公式系统设计
================================================================================

3.1 支持的 LTL 算子
-------------------

| 算子     | 符号        | 含义           | 示例                    |
|----------|-------------|----------------|-------------------------|
| Always   | G / □       | 全局/永远      | G(!wall) 永不撞墙       |
| Eventually| F / ◇      | 最终/终将      | F(goal) 最终到达目标    |
| Until    | U           | 直到           | (!a) U b                |
| Next     | X           | 下一步         | X(safe)                 |
| And      | & / ∧       | 与             | G(!wall) & G(!boundary) |
| Or       | | / ∨       | 或             | F(a) | F(b)             |
| Not      | ! / ¬       | 非             | !wall                   |
| Implies  | ->          | 蕴含           | danger -> X(!danger)    |

3.2 原子命题系统
----------------

分为两类：

A. 环境自动提供的基础命题（auto）
   - wall：agent 当前位置是墙（或下一步会撞墙）
   - goal：agent 当前位置是目标
   - boundary：agent 越界
   - empty：agent 当前位置是空地

B. 用户自定义命题
   - position 类型：指定坐标位置
   - region 类型：指定区域（多个坐标）
   - 未来可扩展：条件表达式等

3.3 配置文件示例
----------------

```yaml
# config.yaml

# 原子命题定义
atomic_propositions:
  # 环境自动提供
  wall: "auto"
  goal: "auto"
  boundary: "auto"
  
  # 用户自定义
  checkpoint1:
    type: "position"
    pos: [2, 3]
  danger_zone:
    type: "region"
    positions: [[1,1], [1,2], [2,1]]

# 安全约束（形式化保证）
safety:
  enabled: true
  formula: "G(!wall) & G(!boundary) & G(!danger_zone)"

# 任务设置（RL 学习）
task:
  type: "reach_goal"
  use_robustness_reward: true  # 使用基于距离的奖励塑形
```


四、Robustness（鲁棒度）计算
================================================================================

4.1 核心思想
------------
Robustness ρ 是 LTL 公式满足程度的量化度量：
- ρ > 0：满足公式，值越大满足得越"好"
- ρ < 0：违反公式，值越小违反得越"严重"
- ρ = 0：临界状态

4.2 离散环境的 Robustness 定义
------------------------------

虽然我们的环境是离散的，但可以用距离定义连续的 robustness：

原子命题 robustness：
- ρ(wall) = min_distance_to_wall - 1
  距离墙 >= 1 格时安全（ρ >= 0）
  
- ρ(goal) = -distance_to_goal
  在目标位置时 ρ = 0，越远 ρ 越负
  
- ρ(region) = -min_distance_to_region（如果要进入）
            = min_distance_to_region - 1（如果要避开）

复合公式 robustness（递归定义）：
- ρ(φ1 & φ2) = min(ρ(φ1), ρ(φ2))
- ρ(φ1 | φ2) = max(ρ(φ1), ρ(φ2))
- ρ(!φ) = -ρ(φ)
- ρ(G(φ)) = min over trajectory (ρ(φ))
- ρ(F(φ)) = max over trajectory (ρ(φ))

4.3 Robustness 的用途
---------------------

1. 安全判断：ρ(safety_formula) > 0 表示安全
2. 奖励塑形：将 ρ 作为奖励的一部分，提供连续反馈
3. 动作选择：在多个安全动作中选择 ρ 最大的


五、FSA（有限状态自动机）模块
================================================================================

5.1 FSA 的作用
--------------

将 LTL 公式转换为有限状态自动机，用于：
1. 运行时监控：跟踪公式满足状态
2. 检测违规：识别进入 trap state（不可恢复的违规）
3. 进度追踪：对于顺序任务，追踪完成进度

5.2 FSA-augmented MDP
---------------------

将自动机状态加入 RL 状态空间：
- 原始状态：s = (agent_pos)
- 增强状态：s' = (agent_pos, automaton_state)

好处：
- Agent 知道当前任务进度
- 可以根据自动机状态给予阶段性奖励

5.3 实现方式
------------

使用 Spot 库：
1. 解析 LTL 公式
2. 生成 Büchi 自动机
3. 运行时模拟自动机转移


六、奖励函数设计
================================================================================

6.1 奖励组成
------------

r_total = r_task + r_safety + r_robustness + r_step

- r_task：任务相关奖励
  - 到达目标：+10
  - 基于距离的塑形奖励（可选）
  
- r_safety：安全相关奖励
  - 被安全层干预：-1（惩罚尝试不安全动作）
  - 实际撞墙（无保护时）：-5
  
- r_robustness：基于 robustness 的连续奖励
  - 可以是 ρ(safety_formula) 的函数
  - 鼓励远离危险边界
  
- r_step：每步惩罚
  - -0.1（鼓励快速完成）

6.2 奖励塑形策略
----------------

对于 Safety 公式 G(!wall)：
- 不直接用 robustness 做奖励（因为有硬约束保护）
- 只在被干预时给惩罚

对于 Task 目标 F(goal)：
- 使用基于距离的 robustness 做奖励塑形
- 提供连续的学习信号


七、文件结构与模块划分
================================================================================

safe_rl_drone/
├── __init__.py
├── env.py              # GridWorld 环境（扩展支持自定义命题）
├── ltl/
│   ├── __init__.py
│   ├── parser.py       # LTL 公式解析（使用 Spot）
│   ├── robustness.py   # Robustness 计算
│   ├── fsa.py          # FSA 生成与监控
│   └── propositions.py # 原子命题管理
├── safety/
│   ├── __init__.py
│   ├── monitor.py      # 安全监控器
│   └── action_filter.py# 动作过滤/替换
├── wrappers/
│   ├── __init__.py
│   └── safe_wrapper.py # 安全包装器（整合各模块）
└── utils/
    ├── __init__.py
    └── map_loader.py   # 地图加载（支持自定义标记）

config.yaml             # 配置文件
maps/
├── map1.txt           # 基础地图
├── map2.txt           # 带检查点的地图
└── ...


八、实现计划
================================================================================

阶段 1：基础架构重构
--------------------
1. 重构 ltl_safety.py → ltl/ 模块
2. 实现 LTL 公式解析器（支持完整语法）
3. 实现原子命题管理系统
4. 扩展 config.yaml 格式

阶段 2：Robustness 系统
-----------------------
1. 实现基于距离的原子命题 robustness
2. 实现复合公式 robustness 递归计算
3. 集成到奖励函数

阶段 3：FSA 模块
----------------
1. 使用 Spot 生成 FSA
2. 实现运行时自动机模拟
3. 实现 FSA-augmented MDP（可选）

阶段 4：安全层增强
------------------
1. 重构 safe_wrapper.py
2. 支持多种安全公式
3. 优化动作替换策略

阶段 5：测试与验证
------------------
1. 设计多种测试场景
2. 对比实验：有/无安全保护，有/无 robustness 奖励
3. 泛化性测试：不同地图、不同目标位置


九、预期成果
================================================================================

1. 一个通用的 Safe RL 框架
   - 用户可以用 LTL 公式定义任意安全约束
   - 安全约束在训练和部署时都得到保证

2. 可解释的安全保证
   - 形式化方法提供数学上的安全保证
   - 可以解释为什么某个动作被阻止

3. 良好的泛化性能
   - 任务策略由 RL 学习，不被硬编码
   - 可以适应不同的目标位置和地图

4. 学术价值
   - 结合形式化方法和强化学习的前沿方向
   - 可以作为毕业论文/研究项目的基础


十、参考文献
================================================================================

1. Li, X., Vasile, C. I., & Belta, C. (2017). 
   Reinforcement learning with temporal logic rewards. 
   IROS 2017.

2. Li, X., et al. (2019). 
   A formal methods approach to interpretable reinforcement learning 
   for robotic planning. 
   Science Robotics.

3. Hasanbeig, M., et al. (2020). 
   Deep reinforcement learning with temporal logics.
   ICML 2020.

================================================================================
文档版本：v1.0
创建日期：2026-01-04
================================================================================
